---
title: "Value of Local Showrooms to Online Competitors: Causal Forest"
author: "Beyza Celik"
date: "9/23/2020"
output: pdf_document
---

```{r setup, include=FALSE}
rm(list = ls())
library(magrittr)
library(tidyverse)
library(haven)
library(zoo)
library(plm)
library(stargazer)
library(grf)
knitr::opts_chunk$set(echo = TRUE)
```

```{r data paths}
data.dir <- "data"
bb_zipcode_path <- file.path(data.dir,"bestbuyzipcodes_sample.sas7bdat")
sales_cc_0mile_path <- file.path(data.dir,"sales_ccity0milezipcode_sample.sas7bdat")
sales_cc_5miles_path <- file.path(data.dir,"sales_ccity5milezipcode_sample.sas7bdat")
sales_allother_zipcode_path <- file.path(data.dir,"sales_allotherzipcode_sample.sas7bdat")
```


```{r load data}
bb_zipcode <- read_sas(bb_zipcode_path) %>% 
  mutate_all(~as.factor(.))

sales_cc_0mile <- read_sas(sales_cc_0mile_path) %>% 
  mutate_at(vars(-pages_viewed, -duration,-event_date, -event_time,
                 -prod_name, -prod_qty, -prod_totprice, -basket_tot, 
                 -household_size, -hoh_oldest_age, -household_income, -children), as.factor)

sales_cc_5miles <- read_sas(sales_cc_5miles_path) %>% 
  mutate_at(vars(-pages_viewed, -duration,-event_date, -event_time,
                 -prod_name, -prod_qty, -prod_totprice, -basket_tot, 
                 -household_size, -hoh_oldest_age, -household_income, -children), as.factor)

# use encoding="latin1" in mac and linux, otherwise you don't need it
sales_allother_zipcode <- read_sas(sales_allother_zipcode_path, encoding = "latin1") %>% 
  mutate(Store_Close_Status = 0) %>% # NaN means no CC in 5-miles radius, we change NaN to 0
  mutate_at(vars(-pages_viewed, -duration,-event_date, -event_time,
                 -prod_name, -prod_qty, -prod_totprice, -basket_tot, 
                 -household_size, -hoh_oldest_age, -household_income, -children), as.factor)
```

```{r}
# Exclude Data without purchase
# All data should be with purchase -> tran_flg == 1
# tran_flg=1 in all datasets already we do not need to filter 
concat_data1 <- rbind(sales_allother_zipcode, sales_cc_0mile)
concat_data2 <- rbind(sales_allother_zipcode, sales_cc_5miles)
```


```{r}
# Filter Referring Domain

# we identify some search engines

ref_domain_to_consider1 <- c("", "GOOGLE.COM", "YAHOO.COM", "google.com", "yahoo.com",
                             "MSN.COM", "msn.com", "aol.com", "AOL.COM", "LIVE.COM", "live.com",
                             "MYWEBSEARCH.COM", "ASK.COM", "MYWAY.COM", "mywebsearch.com",
                             "ask.com", "YAHOO.NET", "BIZRATE.COM", "bizrate.com")

# Then we filter data by refer domain name
concat_data1 %<>% filter(ref_domain_name %in% ref_domain_to_consider1)
concat_data2 %<>% filter(ref_domain_name %in% ref_domain_to_consider1)
```

```{r}
# Filter Target Domain Name
five_target_domain_to_consider <- c("amazon.com", "staples.com", "dell.com", "walmart.com", "bestbuy.com")
two_target_domain_to_consider <- c("amazon.com","bestbuy.com")

# we can choose what filter to apply
concat_data1 %<>% filter(domain_name %in% five_target_domain_to_consider)
concat_data2 %<>% filter(domain_name %in% five_target_domain_to_consider)
```

```{r}
# Product Categories
# 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40
# remove 28, 30, 39, 40
# all_data$prod_category_id %>% unique() %>% sort
category_to_consider <- c(22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37)
experience_product <- c(24, 25, 26, 27, 28, 31, 32, 33, 34, 36, 37)
search_product <- c(22, 23, 24, 29, 30, 35)

concat_data1 %<>% 
  filter(prod_category_id %in% category_to_consider) %>% 
  mutate(prod_category_type = ifelse(prod_category_id %in% experience_product, 1, 0) %>% 
           as.factor()) #experience proudct=1, search product=0

concat_data2 %<>% 
  filter(prod_category_id %in% category_to_consider) %>% 
  mutate(prod_category_type = ifelse(prod_category_id %in% experience_product, 1, 0) %>% 
           as.factor()) #experience proudct=1, search product=0


```

```{r}
# construct MonthYear - month of year
concat_data1 %<>% mutate(MonthYear = format(event_date, "%Y-%m") )
concat_data2 %<>% mutate(MonthYear = format(event_date, "%Y-%m") )
```

```{r}
# Mark CC Closure

# CCStorePresent
# it is the same as Store_Close_Status
concat_data1 %<>% mutate(CCStorePresent = Store_Close_Status)
concat_data2 %<>% mutate(CCStorePresent = Store_Close_Status)
```

```{r}
# AfterStoreClosing
concat_data1 %<>% mutate(AfterStoreClosing = ifelse(MonthYear < "2008-11", 0, 1) %>% 
                           as.factor())
concat_data2 %<>% mutate(AfterStoreClosing = ifelse(MonthYear < "2008-11", 0, 1) %>% 
                           as.factor())  
```

```{r}
# BBStorePresent
concat_data1 %<>% 
  merge(.,bb_zipcode, by.x ="Zip_Code", by.y = "Zip_Code", all.x = TRUE) %>% 
  mutate(BBStorePresent = ifelse(is.na(BB_Store_Status), 0, BB_Store_Status) %>% 
           as.factor,
         PagesPerDollar = pages_viewed/prod_totprice,
         MinsPerDollar = duration/prod_totprice) 

concat_data2 %<>% 
  merge(.,bb_zipcode, by.x ="Zip_Code", by.y = "Zip_Code", all.x = TRUE) %>% 
  mutate(BBStorePresent = ifelse(is.na(BB_Store_Status), 0, BB_Store_Status) %>% 
           as.factor,
         PagesPerDollar = pages_viewed/prod_totprice,
         MinsPerDollar = duration/prod_totprice)
```

# Sales Effect
## Amazon Sales Effect using Zero Mile Data
```{r}


cf_d1 <- concat_data1 %>% 
   mutate(treatment=ifelse(CCStorePresent==1 & AfterStoreClosing==1, 1, 0)) %>% 
  select(-Store_Close_Status, -domain_id, -ref_domain_name, -MinsPerDollar,
         -event_date,-event_time,-tran_flg,-prod_name, -MonthYear,
         -CCStorePresent,- AfterStoreClosing,-BB_Store_Status, -PagesPerDollar,
         -site_session_id, -prod_category_id, -basket_tot, -machine_id, -Zip_Code)

cf_d2 <- concat_data2  %>%
  mutate(treatment=ifelse(CCStorePresent==1 & AfterStoreClosing==1, 1, 0)) %>% 
  select(-Store_Close_Status,-domain_id, -ref_domain_name, -MinsPerDollar,
         -event_date,-event_time,-tran_flg,-prod_name, -MonthYear,
         -CCStorePresent, -AfterStoreClosing,-BB_Store_Status, -PagesPerDollar,
         -site_session_id,-prod_category_id, -basket_tot, -machine_id, -Zip_Code)

```

```{r}
set.seed(1)

ama_cf_d1 <- cf_d1 %>% 
  filter(domain_name=="amazon.com") %>% 
  select(-domain_name)

W1_ama <- ama_cf_d1$treatment 
Y1_ama <- {ama_cf_d1$prod_totprice +1}%>% log(.)

d1_ama <- ama_cf_d1 %>% 
  select(-pages_viewed, -duration, -prod_qty, 
         -prod_totprice, -household_size, 
         -hoh_oldest_age, -household_income, 
         -children, -treatment)

d1_ama_exp <-model.matrix(~.+0, d1_ama)


# ama_cf_d1[,-"prod_totprice"]
X1_ama <- cbind(ama_cf_d1[,-c(4, 16, which(colnames(ama_cf_d1) %in% colnames(d1_ama)))], d1_ama_exp)

Y1_f_ama <- regression_forest(X1_ama, Y1_ama)
Y1_hat_ama <- predict(Y1_f_ama)$predictions

W1_f_ama <- regression_forest(X1_ama, W1_ama)
W1_hat_ama <- predict(W1_f_ama)$predictions

cf1_raw_ama <- causal_forest(X1_ama, Y1_ama, W1_ama,
                       Y.hat = Y1_hat_ama, W.hat = W1_hat_ama)

varimp1_ama <- variable_importance(cf1_raw_ama)
selected1_idx_ama <- which(varimp1_ama > mean(varimp1_ama))

cf1_ama <- causal_forest(X1_ama[,selected1_idx_ama], Y1_ama, W1_ama,
                   Y.hat = Y1_hat_ama, W.hat = W1_hat_ama,
                   tune.parameters = "all")

tau1_hat_ama <- predict(cf1_ama)$predictions

```

```{r}
#
# Estimate ATE
#

ATE1_ama <- average_treatment_effect(cf1_ama)
paste("95% CI for the ATE:", round(ATE1_ama[1], 3),
      "+/-", round(qnorm(0.975) * ATE1_ama[2], 3))

#
# Omnibus tests for heterogeneity
#

# Run best linear predictor analysis
test_calibration(cf1_ama)

# Compare regions with high and low estimated CATEs
high1_effect_ama <- tau1_hat_ama > median(tau1_hat_ama)
ate1.high_ama = average_treatment_effect(cf1_ama, subset = high1_effect_ama, target.sample = "overlap")
ate1_low_ama = average_treatment_effect(cf1_ama, subset = !high1_effect_ama, target.sample = "overlap")
paste("95% CI for difference in ATE:",
      round(ate1.high_ama[1] - ate1_low_ama[1], 3), "+/-",
      round(qnorm(0.975) * sqrt(ate1.high_ama[2]^2 + ate1_low_ama[2]^2), 3))


```

```{r}
#
# Analaysis without fitting the propensity score
#

cf1_ama_noprop = causal_forest(X1_ama[,selected1_idx_ama], Y1_ama, W1_ama,
                          Y.hat = Y1_hat_ama, W.hat = mean(W1_ama),
                          tune.parameters = "all")
tau1_hat_ama_noprop = predict(cf1_ama_noprop)$predictions

ATE1_ama_noprop = average_treatment_effect(cf1_ama_noprop)
paste("95% CI for the ATE:", round(ATE1_ama_noprop[1], 3),
      "+/-", round(qnorm(0.975) * ATE1_ama_noprop[2], 3))

# pdf("tauhat1_ama_noprop.pdf")
# pardef = par(mar = c(5, 4, 4, 2) + 0.5, cex.lab=1.5, cex.axis=1.5, cex.main=1.5, cex.sub=1.5)
plot(tau1_hat_ama, tau1_hat_ama_noprop,
     xlim = range(tau1_hat_ama, tau1_hat_ama_noprop),
     ylim = range(tau1_hat_ama, tau1_hat_ama_noprop),
     xlab = "orthogonalized causal forest estimates",
     ylab = "non-orthogonalized causal forest")
abline(0, 1, lwd = 2, lty = 2, col = 4)
# par = pardef
# dev.off()




```

```{r}
#
# Make some plots...
#

# pdf("tauhat1_ama_hist.pdf")
# pardef = par(mar = c(5, 4, 4, 2) + 0.5, cex.lab=1.5, cex.axis=1.5, cex.main=1.5, cex.sub=1.5)
hist(tau1_hat_ama, xlab = "estimated CATE", main = "")
# dev.off()

# pdf("tauhat1_ama_hist_noprop.pdf")
# pardef = par(mar = c(5, 4, 4, 2) + 0.5, cex.lab=1.5, cex.axis=1.5, cex.main=1.5, cex.sub=1.5)
hist(tau1_hat_ama_noprop, xlab = "estimated CATE", main = "")
# dev.off()

# pdf("tauhat1_ama_vs_dur.pdf")
# pardef = par(mar = c(5, 4, 4, 2) + 0.5, cex.lab=1.5, cex.axis=1.5, cex.main=1.5, cex.sub=1.5)
# plot(tau1_hat_ama ~ round(X1_ama$pages_viewed), xlab = "page viewed", ylab = "estimated CATE")
# lines(smooth.spline(4 + X1_ama[,"pages_viewed"], tau1_hat_ama, df = 4), lwd = 2, col = 4)
# dev.off()
```
## Amazon Sales Effect using Five Miles Data
```{r}
set.seed(1)

ama_cf_d2 <- cf_d2 %>% 
  filter(domain_name=="amazon.com") %>% 
  select(-domain_name)

W2_ama <- ama_cf_d2$treatment 
Y2_ama <- {ama_cf_d2$prod_totprice +1}%>% log(.)

d2_ama <- ama_cf_d2 %>% 
  select(-pages_viewed, -duration, -prod_qty, 
         -prod_totprice, -household_size, 
         -hoh_oldest_age, -household_income, 
         -children, -treatment)

d2_ama_exp <-model.matrix(~.+0, d2_ama)

X2_ama <- cbind(ama_cf_d2[,-c(4, 16, which(colnames(ama_cf_d2) %in% colnames(d2_ama)))], d2_ama_exp)

Y2_f_ama <- regression_forest(X2_ama, Y2_ama)
Y2_hat_ama <- predict(Y2_f_ama)$predictions

W2_f_ama <- regression_forest(X2_ama, W2_ama)
W2_hat_ama <- predict(W2_f_ama)$predictions

cf2_raw_ama <- causal_forest(X2_ama, Y2_ama, W2_ama,
                       Y.hat = Y2_hat_ama, W.hat = W2_hat_ama)

varimp2_ama <- variable_importance(cf2_raw_ama)
selected2_idx_ama <- which(varimp2_ama > mean(varimp2_ama))

cf2_ama <- causal_forest(X2_ama[,selected2_idx_ama], Y2_ama, W2_ama,
                   Y.hat = Y2_hat_ama, W.hat = W2_hat_ama,
                   tune.parameters = "all")

tau2_hat_ama <- predict(cf2_ama)$predictions

```

```{r}
#
# Estimate ATE
#

ATE2_ama <- average_treatment_effect(cf2_ama, target.sample = "overlap")
paste("95% CI for the ATE:", round(ATE2_ama[1], 3),
      "+/-", round(qnorm(0.975) * ATE2_ama[2], 3))

#
# Omnibus tests for heterogeneity
#

# Run best linear predictor analysis
test_calibration(cf2_ama)

# Compare regions with high and low estimated CATEs
high2_effect_ama = tau2_hat_ama > median(tau2_hat_ama)
ate2.high_ama = average_treatment_effect(cf2_ama, subset = high2_effect_ama, target.sample = "overlap")
ate2_low_ama = average_treatment_effect(cf2_ama, subset = !high2_effect_ama, target.sample = "overlap")
paste("95% CI for difference in ATE:",
      round(ate2.high_ama[1] - ate2_low_ama[1], 3), "+/-",
      round(qnorm(0.975) * sqrt(ate2.high_ama[2]^2 + ate2_low_ama[2]^2), 3))


```

```{r}
#
# Analaysis without fitting the propensity score
#

cf2_ama_noprop = causal_forest(X2_ama[,selected2_idx_ama], Y2_ama, W2_ama,
                          Y.hat = Y2_hat_ama, W.hat = mean(W2_ama),
                          tune.parameters = "all")
tau2_hat_ama_noprop = predict(cf2_ama_noprop)$predictions

ATE2_ama_noprop = average_treatment_effect(cf2_ama_noprop)
paste("95% CI for the ATE:", round(ATE2_ama_noprop[1], 3),
      "+/-", round(qnorm(0.975) * ATE2_ama_noprop[2], 3))

# pdf("tauhat2_ama_noprop.pdf")
# pardef = par(mar = c(5, 4, 4, 2) + 0.5, cex.lab=1.5, cex.axis=1.5, cex.main=1.5, cex.sub=1.5)
plot(tau2_hat_ama, tau2_hat_ama_noprop,
     xlim = range(tau2_hat_ama, tau2_hat_ama_noprop),
     ylim = range(tau2_hat_ama, tau2_hat_ama_noprop),
     xlab = "orthogonalized causal forest estimates",
     ylab = "non-orthogonalized causal forest")
abline(0, 1, lwd = 2, lty = 2, col = 4)
# par = pardef
# dev.off()




```

```{r}
#
# Make some plots...
#

# pdf("tauhat2_ama_hist.pdf")
# pardef = par(mar = c(5, 4, 4, 2) + 0.5, cex.lab=1.5, cex.axis=1.5, cex.main=1.5, cex.sub=1.5)
hist(tau2_hat_ama, xlab = "estimated CATE", main = "")
# dev.off()

# pdf("tauhat2_ama_hist_noprop.pdf")
# pardef = par(mar = c(5, 4, 4, 2) + 0.5, cex.lab=1.5, cex.axis=1.5, cex.main=1.5, cex.sub=1.5)
hist(tau2_hat_ama_noprop, xlab = "estimated CATE", main = "")
# dev.off()

# pdf("tauhat2_ama_vs_pagesview.pdf")
# pardef = par(mar = c(5, 4, 4, 2) + 0.5, cex.lab=1.5, cex.axis=1.5, cex.main=1.5, cex.sub=1.5)
# plot(tau2_hat_ama ~ round(X2_ama$duration), xlab = "duration", ylab = "estimated CATE")
# lines(smooth.spline(4 + X2_ama[,"duration"], tau2_hat_ama, df = 4), lwd = 2, col = 4)
# dev.off()
```
## Best Buy Sales Effect using Zero Mile Data
```{r}
set.seed(1)

bb_cf_d1 <- cf_d1 %>% 
  filter(domain_name=="bestbuy.com") %>% 
  select(-domain_name)

W1_bb <- bb_cf_d1$treatment 
Y1_bb <- {bb_cf_d1$prod_totprice +1}%>% log(.)

d1_bb <- bb_cf_d1 %>% 
  select(-pages_viewed, -duration, -prod_qty, 
         -prod_totprice, -household_size, 
         -hoh_oldest_age, -household_income, 
         -children, -treatment)

d1_bb_exp <-model.matrix(~.+0, d1_bb)

X1_bb <- cbind(bb_cf_d1[,-c(4, 16, which(colnames(bb_cf_d1) %in% colnames(d1_bb)))], d1_bb_exp)

Y1_f_bb <- regression_forest(X1_bb, Y1_bb)
Y1_hat_bb <- predict(Y1_f_bb)$predictions

W1_f_bb <- regression_forest(X1_bb, W1_bb)
W1_hat_bb <- predict(W1_f_bb)$predictions

cf1_raw_bb <- causal_forest(X1_bb, Y1_bb, W1_bb,
                       Y.hat = Y1_hat_bb, W.hat = W1_hat_bb)

varimp1_bb <- variable_importance(cf1_raw_bb)
selected1_idx_bb <- which(varimp1_bb > mean(varimp1_bb))

cf1_bb <- causal_forest(X1_bb[,selected1_idx_bb], Y1_bb, W1_bb,
                   Y.hat = Y1_hat_bb, W.hat = W1_hat_bb,
                   tune.parameters = "all")

tau1_hat_bb <- predict(cf1_bb)$predictions


```

```{r}
#
# Estimate ATE
#

ATE1_bb = average_treatment_effect(cf1_bb, target.sample = "overlap")
paste("95% CI for the ATE:", round(ATE1_bb[1], 3),
      "+/-", round(qnorm(0.975) * ATE1_bb[2], 3))

#
# Omnibus tests for heterogeneity
#

# Run best linear predictor analysis
test_calibration(cf1_bb)

# Compare regions with high and low estimated CATEs
high1_effect_bb = tau1_hat_bb > median(tau1_hat_bb)
ate1.high_bb = average_treatment_effect(cf1_bb, subset = high1_effect_bb, target.sample = "overlap")
ate1_low_bb = average_treatment_effect(cf1_bb, subset = !high1_effect_bb, target.sample = "overlap")
paste("95% CI for difference in ATE:",
      round(ate1.high_ama[1] - ate1_low_bb[1], 3), "+/-",
      round(qnorm(0.975) * sqrt(ate1.high_bb[2]^2 + ate1_low_bb[2]^2), 3))


```

```{r}
#
# Analaysis without fitting the propensity score
#

cf1_bb_noprop = causal_forest(X1_bb[,selected1_idx_bb], Y1_bb, W1_bb,
                          Y.hat = Y1_hat_bb, W.hat = mean(W1_bb),
                          tune.parameters = "all")
tau1_hat_bb_noprop = predict(cf1_bb_noprop)$predictions

ATE1_bb_noprop = average_treatment_effect(cf1_bb_noprop)
paste("95% CI for the ATE:", round(ATE1_bb_noprop[1], 3),
      "+/-", round(qnorm(0.975) * ATE1_bb_noprop[2], 3))

# pdf("tauhat1_bb_noprop.pdf")
# pardef = par(mar = c(5, 4, 4, 2) + 0.5, cex.lab=1.5, cex.axis=1.5, cex.main=1.5, cex.sub=1.5)
plot(tau1_hat_bb, tau1_hat_bb_noprop,
     xlim = range(tau1_hat_bb, tau1_hat_bb_noprop),
     ylim = range(tau1_hat_bb, tau1_hat_bb_noprop),
     xlab = "orthogonalized causal forest estimates",
     ylab = "non-orthogonalized causal forest")
abline(0, 1, lwd = 2, lty = 2, col = 4)
# par = pardef
# dev.off()




```

```{r}
#
# Make some plots...
#

# pdf("tauhat1_bb_hist.pdf")
# pardef = par(mar = c(5, 4, 4, 2) + 0.5, cex.lab=1.5, cex.axis=1.5, cex.main=1.5, cex.sub=1.5)
hist(tau1_hat_bb, xlab = "estimated CATE", main = "")
# dev.off()

# pdf("tauhat1_bb_hist_noprop.pdf")
# pardef = par(mar = c(5, 4, 4, 2) + 0.5, cex.lab=1.5, cex.axis=1.5, cex.main=1.5, cex.sub=1.5)
hist(tau1_hat_bb_noprop, xlab = "estimated CATE", main = "")
# dev.off()

# pdf("tauhat1_bb_vs_pagesview.pdf")
# pardef = par(mar = c(5, 4, 4, 2) + 0.5, cex.lab=1.5, cex.axis=1.5, cex.main=1.5, cex.sub=1.5)
plot(tau1_hat_bb ~ round(X1_bb$pages_viewed), xlab = "pages_viewed", ylab = "estimated CATE")
lines(smooth.spline(4 + X1_bb[,"pages_viewed"], tau1_hat_bb, df = 4), lwd = 2, col = 4)
# dev.off()
```
## Best Buy Sales Effect using Five Miles Data
```{r}
set.seed(1)

bb_cf_d2 <- cf_d2 %>% 
  filter(domain_name=="bestbuy.com") %>% 
  select(-domain_name)


W2_bb <- bb_cf_d2$treatment 
Y2_bb <- {bb_cf_d2$prod_totprice +1}%>% log(.)

d2_bb <- bb_cf_d2 %>% 
  select(-pages_viewed, -duration, -prod_qty, 
         -prod_totprice, -household_size, 
         -hoh_oldest_age, -household_income, 
         -children, -treatment)

d2_bb_exp <-model.matrix(~.+0, d2_bb)

X2_bb <- cbind(bb_cf_d2[,-c(4, 16, which(colnames(bb_cf_d2) %in% colnames(d2_bb)))], d2_bb_exp)

Y2_f_bb <- regression_forest(X2_bb, Y2_bb)
Y2_hat_bb <- predict(Y2_f_bb)$predictions

W2_f_bb <- regression_forest(X2_bb, W2_bb)
W2_hat_bb <- predict(W2_f_bb)$predictions

cf2_raw_bb <- causal_forest(X2_bb, Y2_bb, W2_bb,
                       Y.hat = Y2_hat_bb, W.hat = W2_hat_bb)

varimp2_bb <- variable_importance(cf2_raw_bb)
selected2_idx_bb <- which(varimp2_bb > mean(varimp2_bb))

cf2_bb <- causal_forest(X2_bb[,selected2_idx_bb], Y2_bb, W2_bb,
                   Y.hat = Y2_hat_bb, W.hat = W2_hat_bb,
                   tune.parameters = "all")

tau2_hat_bb <- predict(cf2_bb)$predictions


```

```{r}
#
# Estimate ATE
#

ATE2_bb = average_treatment_effect(cf2_bb, target.sample = "overlap")
paste("95% CI for the ATE:", round(ATE2_bb[1], 3),
      "+/-", round(qnorm(0.975) * ATE2_bb[2], 3))

#
# Omnibus tests for heterogeneity
#

# Run best linear predictor analysis
test_calibration(cf2_bb)

# Compare regions with high and low estimated CATEs
high2_effect_bb = tau2_hat_bb > median(tau2_hat_bb)
ate2.high_bb = average_treatment_effect(cf2_bb, subset = high2_effect_bb, target.sample = "overlap")
ate2_low_bb = average_treatment_effect(cf2_bb, subset = !high2_effect_bb, target.sample = "overlap")
paste("95% CI for difference in ATE:",
      round(ate2.high_ama[1] - ate2_low_bb[1], 3), "+/-",
      round(qnorm(0.975) * sqrt(ate2.high_bb[2]^2 + ate2_low_bb[2]^2), 3))


```

```{r}
#
# Analaysis without fitting the propensity score
#

cf2_bb_noprop = causal_forest(X2_bb[,selected2_idx_bb], Y2_bb, W2_bb,
                          Y.hat = Y2_hat_bb, W.hat = mean(W2_bb),
                          tune.parameters = "all")
tau2_hat_bb_noprop = predict(cf2_bb_noprop)$predictions

ATE2_bb_noprop = average_treatment_effect(cf2_bb_noprop)
paste("95% CI for the ATE:", round(ATE2_bb_noprop[1], 3),
      "+/-", round(qnorm(0.975) * ATE2_bb_noprop[2], 3))

# pdf("tauhat2_bb_noprop.pdf")
# pardef = par(mar = c(5, 4, 4, 2) + 0.5, cex.lab=1.5, cex.axis=1.5, cex.main=1.5, cex.sub=1.5)
plot(tau2_hat_bb, tau2_hat_bb_noprop,
     xlim = range(tau2_hat_bb, tau2_hat_bb_noprop),
     ylim = range(tau2_hat_bb, tau2_hat_bb_noprop),
     xlab = "orthogonalized causal forest estimates",
     ylab = "non-orthogonalized causal forest")
abline(0, 1, lwd = 2, lty = 2, col = 4)
# par = pardef
# dev.off()




```

```{r}
#
# Make some plots...
#

# pdf("tauhat2_bb_hist.pdf")
# pardef = par(mar = c(5, 4, 4, 2) + 0.5, cex.lab=1.5, cex.axis=1.5, cex.main=1.5, cex.sub=1.5)
hist(tau2_hat_bb, xlab = "estimated CATE", main = "")
# dev.off()

# pdf("tauhat2_bb_hist_noprop.pdf")
# pardef = par(mar = c(5, 4, 4, 2) + 0.5, cex.lab=1.5, cex.axis=1.5, cex.main=1.5, cex.sub=1.5)
hist(tau2_hat_bb_noprop, xlab = "estimated CATE", main = "")
# dev.off()

# pdf("tauhat2_bb_vs_pagesview.pdf")
# pardef = par(mar = c(5, 4, 4, 2) + 0.5, cex.lab=1.5, cex.axis=1.5, cex.main=1.5, cex.sub=1.5)
plot(tau2_hat_bb ~ round(X2_bb$pages_viewed), xlab = "pages_viewed", ylab = "estimated CATE")
lines(smooth.spline(4 + X2_bb[,"pages_viewed"], tau2_hat_bb, df = 4), lwd = 2, col = 4)
# dev.off()
```
# Online Search Effect
## Search Breadth and Depth
### Amazon Pages Per Dollar using Zero Mile Data
```{r}


cf_d3 <- concat_data1 %>% 
   mutate(treatment=ifelse(CCStorePresent==1 & AfterStoreClosing==1, 1, 0)) %>% 
  select(-Store_Close_Status, -domain_id, -ref_domain_name, -MinsPerDollar,
         -event_date,-event_time,-tran_flg,-prod_name, -MonthYear, -prod_totprice,
         -CCStorePresent,- AfterStoreClosing,-BB_Store_Status, -pages_viewed,
         -site_session_id, -prod_category_id, -basket_tot, -machine_id, -Zip_Code)
```

```{r}
set.seed(1)

ama_cf_d3 <- cf_d3 %>% 
  filter(domain_name=="amazon.com") %>% 
  select(-domain_name)

W3_ama <- ama_cf_d3$treatment 
Y3_ama <- {ama_cf_d3$PagesPerDollar +1}%>% log(.)

d3_ama <- ama_cf_d3 %>% 
  select(-duration, -prod_qty, 
         -household_size, -PagesPerDollar,
         -hoh_oldest_age, -household_income, 
         -children, -treatment)

d3_ama_exp <-model.matrix(~.+0, d3_ama)


# ama_cf_d1[,-"prod_totprice"]
X3_ama <- cbind(ama_cf_d3[,-c(14, 15, which(colnames(ama_cf_d3) %in% colnames(d3_ama)))], d3_ama_exp)

Y3_f_ama <- regression_forest(X3_ama, Y3_ama)
Y3_hat_ama <- predict(Y3_f_ama)$predictions

W3_f_ama <- regression_forest(X3_ama, W3_ama)
W3_hat_ama <- predict(W3_f_ama)$predictions

cf3_raw_ama <- causal_forest(X3_ama, Y3_ama, W3_ama,
                       Y.hat = Y3_hat_ama, W.hat = W3_hat_ama)

varimp3_ama <- variable_importance(cf3_raw_ama)
selected3_idx_ama <- which(varimp3_ama > mean(varimp3_ama))

cf3_ama <- causal_forest(X3_ama[,selected3_idx_ama], Y3_ama, W3_ama,
                   Y.hat = Y3_hat_ama, W.hat = W3_hat_ama,
                   tune.parameters = "all")

tau3_hat_ama <- predict(cf3_ama)$predictions

```

```{r}
#
# Estimate ATE
#

ATE3_ama <- average_treatment_effect(cf3_ama, target.sample = "treated")
paste("95% CI for the ATE:", round(ATE3_ama[1], 3),
      "+/-", round(qnorm(0.975) * ATE3_ama[2], 3))

#
# Omnibus tests for heterogeneity
#

# Run best linear predictor analysis
test_calibration(cf3_ama)

# Compare regions with high and low estimated CATEs
high3_effect_ama <- tau3_hat_ama > median(tau3_hat_ama)
ate3.high_ama = average_treatment_effect(cf3_ama, subset = high3_effect_ama, target.sample = "overlap")
ate3_low_ama = average_treatment_effect(cf3_ama, subset = !high3_effect_ama, target.sample = "overlap")
paste("95% CI for difference in ATE:",
      round(ate3.high_ama[1] - ate3_low_ama[1], 3), "+/-",
      round(qnorm(0.975) * sqrt(ate3.high_ama[2]^2 + ate3_low_ama[2]^2), 3))


```

```{r}
#
# Analaysis without fitting the propensity score
#

cf3_ama_noprop = causal_forest(X3_ama[,selected3_idx_ama], Y3_ama, W3_ama,
                          Y.hat = Y3_hat_ama, W.hat = mean(W3_ama),
                          tune.parameters = "all")
tau3_hat_ama_noprop = predict(cf3_ama_noprop)$predictions

ATE3_ama_noprop = average_treatment_effect(cf3_ama_noprop)
paste("95% CI for the ATE:", round(ATE3_ama_noprop[1], 3),
      "+/-", round(qnorm(0.975) * ATE3_ama_noprop[2], 3))

# pdf("tauhat1_ama_noprop.pdf")
# pardef = par(mar = c(5, 4, 4, 2) + 0.5, cex.lab=1.5, cex.axis=1.5, cex.main=1.5, cex.sub=1.5)
plot(tau3_hat_ama, tau3_hat_ama_noprop,
     xlim = range(tau3_hat_ama, tau3_hat_ama_noprop),
     ylim = range(tau3_hat_ama, tau3_hat_ama_noprop),
     xlab = "orthogonalized causal forest estimates",
     ylab = "non-orthogonalized causal forest")
abline(0, 1, lwd = 2, lty = 2, col = 4)
# par = pardef
# dev.off()




```

```{r}
#
# Make some plots...
#

# pdf("tauhat1_ama_hist.pdf")
# pardef = par(mar = c(5, 4, 4, 2) + 0.5, cex.lab=1.5, cex.axis=1.5, cex.main=1.5, cex.sub=1.5)
hist(tau3_hat_ama, xlab = "estimated CATE", main = "")
# dev.off()

# pdf("tauhat1_ama_hist_noprop.pdf")
# pardef = par(mar = c(5, 4, 4, 2) + 0.5, cex.lab=1.5, cex.axis=1.5, cex.main=1.5, cex.sub=1.5)
hist(tau3_hat_ama_noprop, xlab = "estimated CATE", main = "")
# dev.off()

# pdf("tauhat1_ama_vs_dur.pdf")
# pardef = par(mar = c(5, 4, 4, 2) + 0.5, cex.lab=1.5, cex.axis=1.5, cex.main=1.5, cex.sub=1.5)
# plot(tau1_hat_ama ~ round(X1_ama$pages_viewed), xlab = "page viewed", ylab = "estimated CATE")
# lines(smooth.spline(4 + X1_ama[,"pages_viewed"], tau1_hat_ama, df = 4), lwd = 2, col = 4)
# dev.off()
```

### Amazon Pages Per Dollar using Five Miles Data

```{r}
cf_d4 <- concat_data2  %>%
  mutate(treatment=ifelse(CCStorePresent==1 & AfterStoreClosing==1, 1, 0)) %>% 
  select(-Store_Close_Status,-domain_id, -ref_domain_name, -MinsPerDollar,
         -event_date,-event_time,-tran_flg,-prod_name, -MonthYear, -prod_totprice,
         -CCStorePresent, -AfterStoreClosing,-BB_Store_Status, -pages_viewed,
         -site_session_id,-prod_category_id, -basket_tot, -machine_id, -Zip_Code)
```

```{r}
set.seed(1)

ama_cf_d4 <- cf_d4 %>% 
  filter(domain_name=="amazon.com") %>% 
  select(-domain_name)

W4_ama <- ama_cf_d4$treatment 
Y4_ama <- {ama_cf_d4$PagesPerDollar +1}%>% log(.)

d4_ama <- ama_cf_d4 %>% 
  select(-duration, -prod_qty, 
         -household_size, -PagesPerDollar,
         -hoh_oldest_age, -household_income, 
         -children, -treatment)

d4_ama_exp <-model.matrix(~.+0, d4_ama)


# ama_cf_d1[,-"prod_totprice"]
X4_ama <- cbind(ama_cf_d4[,-c(14, 15, which(colnames(ama_cf_d4) %in% colnames(d4_ama)))], d4_ama_exp)

Y4_f_ama <- regression_forest(X4_ama, Y4_ama)
Y4_hat_ama <- predict(Y4_f_ama)$predictions

W4_f_ama <- regression_forest(X4_ama, W4_ama)
W4_hat_ama <- predict(W4_f_ama)$predictions

cf4_raw_ama <- causal_forest(X4_ama, Y4_ama, W4_ama,
                       Y.hat = Y4_hat_ama, W.hat = W4_hat_ama)

varimp4_ama <- variable_importance(cf4_raw_ama)
selected4_idx_ama <- which(varimp4_ama > mean(varimp4_ama))

cf4_ama <- causal_forest(X4_ama[,selected4_idx_ama], Y4_ama, W4_ama,
                   Y.hat = Y4_hat_ama, W.hat = W4_hat_ama,
                   tune.parameters = "all")

tau4_hat_ama <- predict(cf4_ama)$predictions

```

```{r}
#
# Estimate ATE
#

ATE4_ama <- average_treatment_effect(cf4_ama, target.sample = "treated")
paste("95% CI for the ATE:", round(ATE4_ama[1], 3),
      "+/-", round(qnorm(0.975) * ATE4_ama[2], 3))

#
# Omnibus tests for heterogeneity
#

# Run best linear predictor analysis
test_calibration(cf4_ama)

# Compare regions with high and low estimated CATEs
high4_effect_ama <- tau4_hat_ama > median(tau4_hat_ama)
ate4.high_ama = average_treatment_effect(cf4_ama, subset = high4_effect_ama, target.sample = "overlap")
ate4_low_ama = average_treatment_effect(cf4_ama, subset = !high4_effect_ama, target.sample = "overlap")
paste("95% CI for difference in ATE:",
      round(ate4.high_ama[1] - ate4_low_ama[1], 3), "+/-",
      round(qnorm(0.975) * sqrt(ate4.high_ama[2]^2 + ate4_low_ama[2]^2), 3))


```

```{r}
#
# Analaysis without fitting the propensity score
#

cf4_ama_noprop = causal_forest(X4_ama[,selected4_idx_ama], Y4_ama, W4_ama,
                          Y.hat = Y4_hat_ama, W.hat = mean(W4_ama),
                          tune.parameters = "all")
tau4_hat_ama_noprop = predict(cf4_ama_noprop)$predictions

ATE4_ama_noprop = average_treatment_effect(cf4_ama_noprop)
paste("95% CI for the ATE:", round(ATE4_ama_noprop[1], 3),
      "+/-", round(qnorm(0.975) * ATE4_ama_noprop[2], 3))

# pdf("tauhat1_ama_noprop.pdf")
# pardef = par(mar = c(5, 4, 4, 2) + 0.5, cex.lab=1.5, cex.axis=1.5, cex.main=1.5, cex.sub=1.5)
plot(tau4_hat_ama, tau4_hat_ama_noprop,
     xlim = range(tau4_hat_ama, tau4_hat_ama_noprop),
     ylim = range(tau4_hat_ama, tau4_hat_ama_noprop),
     xlab = "orthogonalized causal forest estimates",
     ylab = "non-orthogonalized causal forest")
abline(0, 1, lwd = 2, lty = 2, col = 4)
# par = pardef
# dev.off()




```

```{r}
#
# Make some plots...
#

# pdf("tauhat1_ama_hist.pdf")
# pardef = par(mar = c(5, 4, 4, 2) + 0.5, cex.lab=1.5, cex.axis=1.5, cex.main=1.5, cex.sub=1.5)
hist(tau4_hat_ama, xlab = "estimated CATE", main = "")
# dev.off()

# pdf("tauhat1_ama_hist_noprop.pdf")
# pardef = par(mar = c(5, 4, 4, 2) + 0.5, cex.lab=1.5, cex.axis=1.5, cex.main=1.5, cex.sub=1.5)
hist(tau4_hat_ama_noprop, xlab = "estimated CATE", main = "")
# dev.off()

# pdf("tauhat1_ama_vs_dur.pdf")
# pardef = par(mar = c(5, 4, 4, 2) + 0.5, cex.lab=1.5, cex.axis=1.5, cex.main=1.5, cex.sub=1.5)
# plot(tau1_hat_ama ~ round(X1_ama$pages_viewed), xlab = "page viewed", ylab = "estimated CATE")
# lines(smooth.spline(4 + X1_ama[,"pages_viewed"], tau1_hat_ama, df = 4), lwd = 2, col = 4)
# dev.off()
```
### Best Buy Pages Per Dollar using Zero Mile Data

```{r}
set.seed(1)

bb_cf_d3 <- cf_d3 %>% 
  filter(domain_name=="bestbuy.com") %>% 
  select(-domain_name)

W3_bb <- bb_cf_d3$treatment 
Y3_bb <- {bb_cf_d3$PagesPerDollar +1}%>% log(.)

d3_bb <- bb_cf_d3 %>% 
  select(-duration, -prod_qty, 
         -household_size, -PagesPerDollar,
         -hoh_oldest_age, -household_income, 
         -children, -treatment)

d3_bb_exp <-model.matrix(~.+0, d3_bb)


# bb_cf_d1[,-"prod_totprice"]
X3_bb <- cbind(bb_cf_d3[,-c(14, 15, which(colnames(bb_cf_d3) %in% colnames(d3_bb)))], d3_bb_exp)

Y3_f_bb <- regression_forest(X3_bb, Y3_bb)
Y3_hat_bb <- predict(Y3_f_bb)$predictions

W3_f_bb <- regression_forest(X3_bb, W3_bb)
W3_hat_bb <- predict(W3_f_bb)$predictions

cf3_raw_bb <- causal_forest(X3_bb, Y3_bb, W3_bb,
                       Y.hat = Y3_hat_bb, W.hat = W3_hat_bb)

varimp3_bb <- variable_importance(cf3_raw_bb)
selected3_idx_bb <- which(varimp3_bb > mean(varimp3_bb))

cf3_bb <- causal_forest(X3_bb[,selected3_idx_bb], Y3_bb, W3_bb,
                   Y.hat = Y3_hat_bb, W.hat = W3_hat_bb,
                   tune.parameters = "all")

tau3_hat_bb <- predict(cf3_bb)$predictions

```

```{r}
#
# Estimate ATE
#

ATE3_bb <- average_treatment_effect(cf3_bb, target.sample = "treated")
paste("95% CI for the ATE:", round(ATE3_bb[1], 3),
      "+/-", round(qnorm(0.975) * ATE3_bb[2], 3))

#
# Omnibus tests for heterogeneity
#

# Run best linear predictor analysis
test_calibration(cf3_bb)

# Compare regions with high and low estimated CATEs
high3_effect_bb <- tau3_hat_bb > median(tau3_hat_bb)
ate3.high_bb = average_treatment_effect(cf3_bb, subset = high3_effect_bb, target.sample = "overlap")
ate3_low_bb = average_treatment_effect(cf3_bb, subset = !high3_effect_bb, target.sample = "overlap")
paste("95% CI for difference in ATE:",
      round(ate3.high_bb[1] - ate3_low_bb[1], 3), "+/-",
      round(qnorm(0.975) * sqrt(ate3.high_bb[2]^2 + ate3_low_bb[2]^2), 3))


```

```{r}
#
# Analaysis without fitting the propensity score
#

cf3_bb_noprop <- causal_forest(X3_bb[,selected3_idx_bb], Y3_bb, W3_bb,
                          Y.hat = Y3_hat_bb, W.hat = mean(W3_bb),
                          tune.parameters = "all")
tau3_hat_bb_noprop <- predict(cf3_bb_noprop)$predictions

ATE3_bb_noprop <- average_treatment_effect(cf3_bb_noprop)
paste("95% CI for the ATE:", round(ATE3_bb_noprop[1], 3),
      "+/-", round(qnorm(0.975) * ATE3_bb_noprop[2], 3))

# pdf("tauhat1_bb_noprop.pdf")
# pardef = par(mar = c(5, 4, 4, 2) + 0.5, cex.lab=1.5, cex.axis=1.5, cex.main=1.5, cex.sub=1.5)
plot(tau3_hat_bb, tau3_hat_bb_noprop,
     xlim = range(tau3_hat_bb, tau3_hat_bb_noprop),
     ylim = range(tau3_hat_bb, tau3_hat_bb_noprop),
     xlab = "orthogonalized causal forest estimates",
     ylab = "non-orthogonalized causal forest")
abline(0, 1, lwd = 2, lty = 2, col = 4)
# par = pardef
# dev.off()




```

```{r}
#
# Make some plots...
#

# pdf("tauhat1_ama_hist.pdf")
# pardef = par(mar = c(5, 4, 4, 2) + 0.5, cex.lab=1.5, cex.axis=1.5, cex.main=1.5, cex.sub=1.5)
hist(tau3_hat_bb, xlab = "estimated CATE", main = "")
# dev.off()

# pdf("tauhat1_bb_hist_noprop.pdf")
# pardef = par(mar = c(5, 4, 4, 2) + 0.5, cex.lab=1.5, cex.axis=1.5, cex.main=1.5, cex.sub=1.5)
hist(tau3_hat_bb_noprop, xlab = "estimated CATE", main = "")
# dev.off()

# pdf("tauhat1_bb_vs_dur.pdf")
# pardef = par(mar = c(5, 4, 4, 2) + 0.5, cex.lab=1.5, cex.axis=1.5, cex.main=1.5, cex.sub=1.5)
# plot(tau1_hat_ama ~ round(X1_ama$pages_viewed), xlab = "page viewed", ylab = "estimated CATE")
# lines(smooth.spline(4 + X1_ama[,"pages_viewed"], tau1_hat_ama, df = 4), lwd = 2, col = 4)
# dev.off()
```

### Best Buy Pages Per Dollar using Five Miles Data

```{r}
set.seed(1)

bb_cf_d4 <- cf_d4 %>% 
  filter(domain_name=="bestbuy.com") %>% 
  select(-domain_name)

W4_bb <- bb_cf_d4$treatment 
Y4_bb <- {bb_cf_d4$PagesPerDollar +1}%>% log(.)

d4_bb <- bb_cf_d4 %>% 
  select(-duration, -prod_qty, 
         -household_size, -PagesPerDollar,
         -hoh_oldest_age, -household_income, 
         -children, -treatment)

d4_bb_exp <-model.matrix(~.+0, d4_bb)


# bb_cf_d1[,-"prod_totprice"]
X4_bb <- cbind(bb_cf_d4[,-c(14, 15, which(colnames(bb_cf_d4) %in% colnames(d4_bb)))], d4_bb_exp)

Y4_f_bb <- regression_forest(X4_bb, Y4_bb)
Y4_hat_bb <- predict(Y4_f_bb)$predictions

W4_f_bb <- regression_forest(X4_bb, W4_bb)
W4_hat_bb <- predict(W4_f_bb)$predictions

cf4_raw_bb <- causal_forest(X4_bb, Y4_bb, W4_bb,
                       Y.hat = Y4_hat_bb, W.hat = W4_hat_bb)

varimp4_bb <- variable_importance(cf4_raw_bb)
selected4_idx_bb <- which(varimp4_bb > mean(varimp4_bb))

cf4_bb <- causal_forest(X4_bb[,selected4_idx_bb], Y4_bb, W4_bb,
                   Y.hat = Y4_hat_bb, W.hat = W4_hat_bb,
                   tune.parameters = "all")

tau4_hat_bb <- predict(cf4_bb)$predictions

```

```{r}
#
# Estimate ATE
#

ATE4_bb <- average_treatment_effect(cf4_bb, target.sample = "treated")
paste("95% CI for the ATE:", round(ATE4_bb[1], 3),
      "+/-", round(qnorm(0.975) * ATE4_bb[2], 3))

#
# Omnibus tests for heterogeneity
#

# Run best linear predictor analysis
test_calibration(cf4_bb)

# Compare regions with high and low estimated CATEs
high4_effect_bb <- tau4_hat_bb > median(tau4_hat_bb)
ate4.high_bb = average_treatment_effect(cf4_bb, subset = high4_effect_bb, target.sample = "overlap")
ate4_low_bb = average_treatment_effect(cf4_bb, subset = !high4_effect_bb, target.sample = "overlap")
paste("95% CI for difference in ATE:",
      round(ate4.high_bb[1] - ate4_low_bb[1], 3), "+/-",
      round(qnorm(0.975) * sqrt(ate4.high_bb[2]^2 + ate4_low_bb[2]^2), 3))


```

```{r}
#
# Analaysis without fitting the propensity score
#

cf4_bb_noprop = causal_forest(X4_bb[,selected4_idx_bb], Y4_bb, W4_bb,
                          Y.hat = Y4_hat_bb, W.hat = mean(W4_bb),
                          tune.parameters = "all")
tau4_hat_bb_noprop = predict(cf4_bb_noprop)$predictions

ATE4_bb_noprop = average_treatment_effect(cf4_bb_noprop)
paste("95% CI for the ATE:", round(ATE4_bb_noprop[1], 3),
      "+/-", round(qnorm(0.975) * ATE4_bb_noprop[2], 3))

# pdf("tauhat1_bb_noprop.pdf")
# pardef = par(mar = c(5, 4, 4, 2) + 0.5, cex.lab=1.5, cex.axis=1.5, cex.main=1.5, cex.sub=1.5)
plot(tau4_hat_bb, tau4_hat_bb_noprop,
     xlim = range(tau4_hat_bb, tau4_hat_bb_noprop),
     ylim = range(tau4_hat_bb, tau4_hat_bb_noprop),
     xlab = "orthogonalized causal forest estimates",
     ylab = "non-orthogonalized causal forest")
abline(0, 1, lwd = 2, lty = 2, col = 4)
# par = pardef
# dev.off()




```

```{r}
#
# Make some plots...
#

# pdf("tauhat1_ama_hist.pdf")
# pardef = par(mar = c(5, 4, 4, 2) + 0.5, cex.lab=1.5, cex.axis=1.5, cex.main=1.5, cex.sub=1.5)
hist(tau4_hat_bb, xlab = "estimated CATE", main = "")
# dev.off()

# pdf("tauhat1_bb_hist_noprop.pdf")
# pardef = par(mar = c(5, 4, 4, 2) + 0.5, cex.lab=1.5, cex.axis=1.5, cex.main=1.5, cex.sub=1.5)
hist(tau4_hat_bb_noprop, xlab = "estimated CATE", main = "")
# dev.off()

# pdf("tauhat1_ama_vs_dur.pdf")
# pardef = par(mar = c(5, 4, 4, 2) + 0.5, cex.lab=1.5, cex.axis=1.5, cex.main=1.5, cex.sub=1.5)
# plot(tau1_hat_ama ~ round(X1_ama$pages_viewed), xlab = "page viewed", ylab = "estimated CATE")
# lines(smooth.spline(4 + X1_ama[,"pages_viewed"], tau1_hat_ama, df = 4), lwd = 2, col = 4)
# dev.off()
```
### Amazon Minutes Per Dollar using Zero Mile Data

```{r}
cf_d5 <- concat_data1 %>% 
   mutate(treatment=ifelse(CCStorePresent==1 & AfterStoreClosing==1, 1, 0)) %>% 
  select(-Store_Close_Status, -domain_id, -ref_domain_name, -PagesPerDollar,
         -event_date,-event_time,-tran_flg,-prod_name, -MonthYear, -prod_totprice,
         -CCStorePresent,- AfterStoreClosing,-BB_Store_Status, -duration,
         -site_session_id, -prod_category_id, -basket_tot, -machine_id, -Zip_Code)
```

```{r}
set.seed(1)

ama_cf_d5 <- cf_d5 %>% 
  filter(domain_name=="amazon.com") %>% 
  select(-domain_name)

W5_ama <- ama_cf_d5$treatment 
Y5_ama <- {ama_cf_d5$MinsPerDollar +1}%>% log(.)

d5_ama <- ama_cf_d5 %>% 
  select(-pages_viewed, -prod_qty, 
         -household_size, -MinsPerDollar,
         -hoh_oldest_age, -household_income, 
         -children, -treatment)

d5_ama_exp <-model.matrix(~.+0, d5_ama)


# ama_cf_d1[,-"prod_totprice"]
X5_ama <- cbind(ama_cf_d5[,-c(14, 15, which(colnames(ama_cf_d5) %in% colnames(d5_ama)))], d5_ama_exp)

Y5_f_ama <- regression_forest(X5_ama, Y5_ama)
Y5_hat_ama <- predict(Y5_f_ama)$predictions

W5_f_ama <- regression_forest(X5_ama, W5_ama)
W5_hat_ama <- predict(W5_f_ama)$predictions

cf5_raw_ama <- causal_forest(X5_ama, Y5_ama, W5_ama,
                       Y.hat = Y5_hat_ama, W.hat = W5_hat_ama)

varimp5_ama <- variable_importance(cf5_raw_ama)
selected5_idx_ama <- which(varimp5_ama > mean(varimp5_ama))

cf5_ama <- causal_forest(X5_ama[,selected5_idx_ama], Y5_ama, W5_ama,
                   Y.hat = Y5_hat_ama, W.hat = W5_hat_ama,
                   tune.parameters = "all")

tau5_hat_ama <- predict(cf5_ama)$predictions

```

```{r}
#
# Estimate ATE
#

ATE5_ama <- average_treatment_effect(cf5_ama, target.sample = "treated")
paste("95% CI for the ATE:", round(ATE5_ama[1], 3),
      "+/-", round(qnorm(0.975) * ATE5_ama[2], 3))

#
# Omnibus tests for heterogeneity
#

# Run best linear predictor analysis
test_calibration(cf5_ama)

# Compare regions with high and low estimated CATEs
high5_effect_ama <- tau5_hat_ama > median(tau5_hat_ama)
ate5.high_ama = average_treatment_effect(cf5_ama, subset = high5_effect_ama, target.sample = "overlap")
ate5_low_ama = average_treatment_effect(cf5_ama, subset = !high5_effect_ama, target.sample = "overlap")
paste("95% CI for difference in ATE:",
      round(ate5.high_ama[1] - ate5_low_ama[1], 3), "+/-",
      round(qnorm(0.975) * sqrt(ate5.high_ama[2]^2 + ate5_low_ama[2]^2), 3))


```

```{r}
#
# Analaysis without fitting the propensity score
#

cf5_ama_noprop <- causal_forest(X5_ama[,selected5_idx_ama], Y5_ama, W5_ama,
                          Y.hat = Y5_hat_ama, W.hat = mean(W5_ama),
                          tune.parameters = "all")
tau5_hat_ama_noprop <- predict(cf5_ama_noprop)$predictions

ATE5_ama_noprop <- average_treatment_effect(cf5_ama_noprop)
paste("95% CI for the ATE:", round(ATE5_ama_noprop[1], 3),
      "+/-", round(qnorm(0.975) * ATE5_ama_noprop[2], 3))

# pdf("tauhat1_ama_noprop.pdf")
# pardef = par(mar = c(5, 4, 4, 2) + 0.5, cex.lab=1.5, cex.axis=1.5, cex.main=1.5, cex.sub=1.5)
plot(tau5_hat_ama, tau5_hat_ama_noprop,
     xlim = range(tau5_hat_ama, tau5_hat_ama_noprop),
     ylim = range(tau5_hat_ama, tau5_hat_ama_noprop),
     xlab = "orthogonalized causal forest estimates",
     ylab = "non-orthogonalized causal forest")
abline(0, 1, lwd = 2, lty = 2, col = 4)
# par = pardef
# dev.off()




```

```{r}
#
# Make some plots...
#

# pdf("tauhat1_ama_hist.pdf")
# pardef = par(mar = c(5, 4, 4, 2) + 0.5, cex.lab=1.5, cex.axis=1.5, cex.main=1.5, cex.sub=1.5)
hist(tau5_hat_ama, xlab = "estimated CATE", main = "")
# dev.off()

# pdf("tauhat1_ama_hist_noprop.pdf")
# pardef = par(mar = c(5, 4, 4, 2) + 0.5, cex.lab=1.5, cex.axis=1.5, cex.main=1.5, cex.sub=1.5)
hist(tau5_hat_ama_noprop, xlab = "estimated CATE", main = "")
# dev.off()

# pdf("tauhat1_ama_vs_dur.pdf")
# pardef = par(mar = c(5, 4, 4, 2) + 0.5, cex.lab=1.5, cex.axis=1.5, cex.main=1.5, cex.sub=1.5)
# plot(tau1_hat_ama ~ round(X1_ama$pages_viewed), xlab = "page viewed", ylab = "estimated CATE")
# lines(smooth.spline(4 + X1_ama[,"pages_viewed"], tau1_hat_ama, df = 4), lwd = 2, col = 4)
# dev.off()
```

### Amazon Minutes Per Dollar using Five Miles Data

```{r}
cf_d6 <- concat_data2 %>% 
   mutate(treatment=ifelse(CCStorePresent==1 & AfterStoreClosing==1, 1, 0)) %>% 
  select(-Store_Close_Status, -domain_id, -ref_domain_name, -PagesPerDollar,
         -event_date,-event_time,-tran_flg,-prod_name, -MonthYear, -prod_totprice,
         -CCStorePresent,- AfterStoreClosing,-BB_Store_Status, -duration,
         -site_session_id, -prod_category_id, -basket_tot, -machine_id, -Zip_Code)
```

```{r}
set.seed(1)

ama_cf_d6 <- cf_d6 %>% 
  filter(domain_name=="amazon.com") %>% 
  select(-domain_name)

W6_ama <- ama_cf_d6$treatment 
Y6_ama <- {ama_cf_d6$MinsPerDollar +1}%>% log(.)

d6_ama <- ama_cf_d6 %>% 
  select(-pages_viewed, -prod_qty, 
         -household_size, -MinsPerDollar,
         -hoh_oldest_age, -household_income, 
         -children, -treatment)

d6_ama_exp <-model.matrix(~.+0, d6_ama)


# ama_cf_d1[,-"prod_totprice"]
X6_ama <- cbind(ama_cf_d6[,-c(14, 15, which(colnames(ama_cf_d6) %in% colnames(d6_ama)))], d6_ama_exp)

Y6_f_ama <- regression_forest(X6_ama, Y6_ama)
Y6_hat_ama <- predict(Y6_f_ama)$predictions

W6_f_ama <- regression_forest(X6_ama, W6_ama)
W6_hat_ama <- predict(W6_f_ama)$predictions

cf6_raw_ama <- causal_forest(X6_ama, Y6_ama, W6_ama,
                       Y.hat = Y6_hat_ama, W.hat = W6_hat_ama)

varimp6_ama <- variable_importance(cf6_raw_ama)
selected6_idx_ama <- which(varimp6_ama > mean(varimp6_ama))

cf6_ama <- causal_forest(X6_ama[,selected6_idx_ama], Y6_ama, W6_ama,
                   Y.hat = Y6_hat_ama, W.hat = W6_hat_ama,
                   tune.parameters = "all")

tau6_hat_ama <- predict(cf6_ama)$predictions

```

```{r}
#
# Estimate ATE
#

ATE6_ama <- average_treatment_effect(cf6_ama, target.sample = "treated")
paste("95% CI for the ATE:", round(ATE6_ama[1], 3),
      "+/-", round(qnorm(0.975) * ATE6_ama[2], 3))

#
# Omnibus tests for heterogeneity
#

# Run best linear predictor analysis
test_calibration(cf6_ama)

# Compare regions with high and low estimated CATEs
high6_effect_ama <- tau6_hat_ama > median(tau6_hat_ama)
ate6.high_ama = average_treatment_effect(cf6_ama, subset = high6_effect_ama, target.sample = "overlap")
ate6_low_ama = average_treatment_effect(cf6_ama, subset = !high6_effect_ama, target.sample = "overlap")
paste("95% CI for difference in ATE:",
      round(ate6.high_ama[1] - ate6_low_ama[1], 3), "+/-",
      round(qnorm(0.975) * sqrt(ate6.high_ama[2]^2 + ate6_low_ama[2]^2), 3))


```

```{r}
#
# Analaysis without fitting the propensity score
#

cf6_ama_noprop = causal_forest(X6_ama[,selected6_idx_ama], Y6_ama, W6_ama,
                          Y.hat = Y6_hat_ama, W.hat = mean(W6_ama),
                          tune.parameters = "all")
tau6_hat_ama_noprop = predict(cf6_ama_noprop)$predictions

ATE6_ama_noprop = average_treatment_effect(cf6_ama_noprop)
paste("95% CI for the ATE:", round(ATE6_ama_noprop[1], 3),
      "+/-", round(qnorm(0.975) * ATE6_ama_noprop[2], 3))

# pdf("tauhat1_ama_noprop.pdf")
# pardef = par(mar = c(5, 4, 4, 2) + 0.5, cex.lab=1.5, cex.axis=1.5, cex.main=1.5, cex.sub=1.5)
plot(tau6_hat_ama, tau6_hat_ama_noprop,
     xlim = range(tau6_hat_ama, tau6_hat_ama_noprop),
     ylim = range(tau6_hat_ama, tau6_hat_ama_noprop),
     xlab = "orthogonalized causal forest estimates",
     ylab = "non-orthogonalized causal forest")
abline(0, 1, lwd = 2, lty = 2, col = 4)
# par = pardef
# dev.off()




```

```{r}
#
# Make some plots...
#

# pdf("tauhat1_ama_hist.pdf")
# pardef = par(mar = c(5, 4, 4, 2) + 0.5, cex.lab=1.5, cex.axis=1.5, cex.main=1.5, cex.sub=1.5)
hist(tau6_hat_ama, xlab = "estimated CATE", main = "")
# dev.off()

# pdf("tauhat1_ama_hist_noprop.pdf")
# pardef = par(mar = c(5, 4, 4, 2) + 0.5, cex.lab=1.5, cex.axis=1.5, cex.main=1.5, cex.sub=1.5)
hist(tau6_hat_ama_noprop, xlab = "estimated CATE", main = "")
# dev.off()

# pdf("tauhat1_ama_vs_dur.pdf")
# pardef = par(mar = c(5, 4, 4, 2) + 0.5, cex.lab=1.5, cex.axis=1.5, cex.main=1.5, cex.sub=1.5)
# plot(tau1_hat_ama ~ round(X1_ama$pages_viewed), xlab = "page viewed", ylab = "estimated CATE")
# lines(smooth.spline(4 + X1_ama[,"pages_viewed"], tau1_hat_ama, df = 4), lwd = 2, col = 4)
# dev.off()
```

### Best Buy Minutes Per Dollar using Zero Mile Data


```{r}
set.seed(1)

bb_cf_d5 <- cf_d5 %>% 
  filter(domain_name=="bestbuy.com") %>% 
  select(-domain_name)

W5_bb <- bb_cf_d5$treatment 
Y5_bb <- {bb_cf_d5$MinsPerDollar +1}%>% log(.)

d5_bb <- bb_cf_d5 %>% 
  select(-pages_viewed, -prod_qty, 
         -household_size, -MinsPerDollar,
         -hoh_oldest_age, -household_income, 
         -children, -treatment)

d5_bb_exp <-model.matrix(~.+0, d5_bb)


# bb_cf_d1[,-"prod_totprice"]
X5_bb <- cbind(bb_cf_d5[,-c(14, 15, which(colnames(bb_cf_d5) %in% colnames(d5_bb)))], d5_bb_exp)

Y5_f_bb <- regression_forest(X5_bb, Y5_bb)
Y5_hat_bb <- predict(Y5_f_bb)$predictions

W5_f_bb <- regression_forest(X5_bb, W5_bb)
W5_hat_bb <- predict(W5_f_bb)$predictions

cf5_raw_bb <- causal_forest(X5_bb, Y5_bb, W5_bb,
                       Y.hat = Y5_hat_bb, W.hat = W5_hat_bb)

varimp5_bb <- variable_importance(cf5_raw_bb)
selected5_idx_bb <- which(varimp5_bb > mean(varimp5_bb))

cf5_bb <- causal_forest(X5_bb[,selected5_idx_bb], Y5_bb, W5_bb,
                   Y.hat = Y5_hat_bb, W.hat = W5_hat_bb,
                   tune.parameters = "all")

tau5_hat_bb <- predict(cf5_bb)$predictions

```

```{r}
#
# Estimate ATE
#

ATE5_bb <- average_treatment_effect(cf5_bb, target.sample = "treated")
paste("95% CI for the ATE:", round(ATE5_bb[1], 3),
      "+/-", round(qnorm(0.975) * ATE5_bb[2], 3))

#
# Omnibus tests for heterogeneity
#

# Run best linear predictor analysis
test_calibration(cf5_bb)

# Compare regions with high and low estimated CATEs
high5_effect_bb <- tau5_hat_bb > median(tau5_hat_bb)
ate5.high_bb = average_treatment_effect(cf5_bb, subset = high5_effect_bb, target.sample = "overlap")
ate5_low_bb = average_treatment_effect(cf5_bb, subset = !high5_effect_bb, target.sample = "overlap")
paste("95% CI for difference in ATE:",
      round(ate5.high_bb[1] - ate5_low_bb[1], 3), "+/-",
      round(qnorm(0.975) * sqrt(ate5.high_bb[2]^2 + ate5_low_bb[2]^2), 3))


```

```{r}
#
# Analaysis without fitting the propensity score
#

cf5_bb_noprop <- causal_forest(X5_bb[,selected5_idx_bb], Y5_bb, W5_bb,
                          Y.hat = Y5_hat_bb, W.hat = mean(W5_bb),
                          tune.parameters = "all")
tau5_hat_bb_noprop <- predict(cf5_bb_noprop)$predictions

ATE5_bb_noprop <- average_treatment_effect(cf5_bb_noprop)
paste("95% CI for the ATE:", round(ATE5_bb_noprop[1], 3),
      "+/-", round(qnorm(0.975) * ATE5_bb_noprop[2], 3))

# pdf("tauhat1_bb_noprop.pdf")
# pardef = par(mar = c(5, 4, 4, 2) + 0.5, cex.lab=1.5, cex.axis=1.5, cex.main=1.5, cex.sub=1.5)
plot(tau5_hat_bb, tau5_hat_bb_noprop,
     xlim = range(tau5_hat_bb, tau5_hat_bb_noprop),
     ylim = range(tau5_hat_bb, tau5_hat_bb_noprop),
     xlab = "orthogonalized causal forest estimates",
     ylab = "non-orthogonalized causal forest")
abline(0, 1, lwd = 2, lty = 2, col = 4)
# par = pardef
# dev.off()




```

```{r}
#
# Make some plots...
#

# pdf("tauhat1_bb_hist.pdf")
# pardef = par(mar = c(5, 4, 4, 2) + 0.5, cex.lab=1.5, cex.axis=1.5, cex.main=1.5, cex.sub=1.5)
hist(tau5_hat_bb, xlab = "estimated CATE", main = "")
# dev.off()

# pdf("tauhat1_bb_hist_noprop.pdf")
# pardef = par(mar = c(5, 4, 4, 2) + 0.5, cex.lab=1.5, cex.axis=1.5, cex.main=1.5, cex.sub=1.5)
hist(tau5_hat_bb_noprop, xlab = "estimated CATE", main = "")
# dev.off()

# pdf("tauhat1_bb_vs_dur.pdf")
# pardef = par(mar = c(5, 4, 4, 2) + 0.5, cex.lab=1.5, cex.axis=1.5, cex.main=1.5, cex.sub=1.5)
# plot(tau1_hat_bb ~ round(X1_bb$pages_viewed), xlab = "page viewed", ylab = "estimated CATE")
# lines(smooth.spline(4 + X1_bb[,"pages_viewed"], tau1_hat_bb, df = 4), lwd = 2, col = 4)
# dev.off()
```

### Best Buy Minutes Per Dollar using Five Miles Data


```{r}
set.seed(1)

bb_cf_d6 <- cf_d6 %>% 
  filter(domain_name=="bestbuy.com") %>% 
  select(-domain_name)

W6_bb <- bb_cf_d6$treatment 
Y6_bb <- {bb_cf_d6$MinsPerDollar +1}%>% log(.)

d6_bb <- bb_cf_d6 %>% 
  select(-pages_viewed, -prod_qty, 
         -household_size, -MinsPerDollar,
         -hoh_oldest_age, -household_income, 
         -children, -treatment)

d6_bb_exp <-model.matrix(~.+0, d6_bb)


# bb_cf_d1[,-"prod_totprice"]
X6_bb <- cbind(bb_cf_d6[,-c(14, 15, which(colnames(bb_cf_d6) %in% colnames(d6_bb)))], d6_bb_exp)

Y6_f_bb <- regression_forest(X6_bb, Y6_bb)
Y6_hat_bb <- predict(Y6_f_bb)$predictions

W6_f_bb <- regression_forest(X6_bb, W6_bb)
W6_hat_bb <- predict(W6_f_bb)$predictions

cf6_raw_bb <- causal_forest(X6_bb, Y6_bb, W6_bb,
                       Y.hat = Y6_hat_bb, W.hat = W6_hat_bb)

varimp6_bb <- variable_importance(cf6_raw_bb)
selected6_idx_bb <- which(varimp6_bb > mean(varimp6_bb))

cf6_bb <- causal_forest(X6_bb[,selected6_idx_bb], Y6_bb, W6_bb,
                   Y.hat = Y6_hat_bb, W.hat = W6_hat_bb,
                   tune.parameters = "all")

tau6_hat_bb <- predict(cf6_bb)$predictions

```

```{r}
#
# Estimate ATE
#

ATE6_bb <- average_treatment_effect(cf6_bb, target.sample = "treated")
paste("95% CI for the ATE:", round(ATE6_bb[1], 3),
      "+/-", round(qnorm(0.975) * ATE6_bb[2], 3))

#
# Omnibus tests for heterogeneity
#

# Run best linear predictor analysis
test_calibration(cf6_bb)

# Compare regions with high and low estimated CATEs
high6_effect_bb <- tau6_hat_bb > median(tau6_hat_bb)
ate6.high_bb = average_treatment_effect(cf6_bb, subset = high6_effect_bb, target.sample = "overlap")
ate6_low_bb = average_treatment_effect(cf6_bb, subset = !high6_effect_bb, target.sample = "overlap")
paste("95% CI for difference in ATE:",
      round(ate6.high_bb[1] - ate6_low_bb[1], 3), "+/-",
      round(qnorm(0.975) * sqrt(ate6.high_bb[2]^2 + ate6_low_bb[2]^2), 3))


```

```{r}
#
# Analaysis without fitting the propensity score
#

cf6_bb_noprop = causal_forest(X6_bb[,selected6_idx_bb], Y6_bb, W6_bb,
                          Y.hat = Y6_hat_bb, W.hat = mean(W6_bb),
                          tune.parameters = "all")
tau6_hat_bb_noprop = predict(cf6_bb_noprop)$predictions

ATE6_bb_noprop = average_treatment_effect(cf6_bb_noprop)
paste("95% CI for the ATE:", round(ATE6_bb_noprop[1], 3),
      "+/-", round(qnorm(0.975) * ATE6_bb_noprop[2], 3))

# pdf("tauhat1_bb_noprop.pdf")
# pardef = par(mar = c(5, 4, 4, 2) + 0.5, cex.lab=1.5, cex.axis=1.5, cex.main=1.5, cex.sub=1.5)
plot(tau6_hat_bb, tau6_hat_bb_noprop,
     xlim = range(tau6_hat_bb, tau6_hat_bb_noprop),
     ylim = range(tau6_hat_bb, tau6_hat_bb_noprop),
     xlab = "orthogonalized causal forest estimates",
     ylab = "non-orthogonalized causal forest")
abline(0, 1, lwd = 2, lty = 2, col = 4)
# par = pardef
# dev.off()




```

```{r}
#
# Make some plots...
#

# pdf("tauhat1_bb_hist.pdf")
# pardef = par(mar = c(5, 4, 4, 2) + 0.5, cex.lab=1.5, cex.axis=1.5, cex.main=1.5, cex.sub=1.5)
hist(tau6_hat_bb, xlab = "estimated CATE", main = "")
# dev.off()

# pdf("tauhat1_bb_hist_noprop.pdf")
# pardef = par(mar = c(5, 4, 4, 2) + 0.5, cex.lab=1.5, cex.axis=1.5, cex.main=1.5, cex.sub=1.5)
hist(tau6_hat_bb_noprop, xlab = "estimated CATE", main = "")
# dev.off()

# pdf("tauhat1_bb_vs_dur.pdf")
# pardef = par(mar = c(5, 4, 4, 2) + 0.5, cex.lab=1.5, cex.axis=1.5, cex.main=1.5, cex.sub=1.5)
# plot(tau1_hat_bb ~ round(X1_bb$pages_viewed), xlab = "page viewed", ylab = "estimated CATE")
# lines(smooth.spline(4 + X1_bb[,"pages_viewed"], tau1_hat_bb, df = 4), lwd = 2, col = 4)
# dev.off()
```
