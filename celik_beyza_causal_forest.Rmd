---
title: "Value of Local Showrooms to Online Competitors: Causal Forest"
author: "Beyza Celik"
date: "9/23/2020"
output: pdf_document
---

```{r setup, include=FALSE}
rm(list = ls())
library(magrittr)
library(tidyverse)
library(haven)
library(zoo)
library(plm)
library(stargazer)
library(grf)
knitr::opts_chunk$set(echo = TRUE)
```

```{r data paths}
data.dir <- "data"
bb_zipcode_path <- file.path(data.dir,"bestbuyzipcodes_sample.sas7bdat")
sales_cc_0mile_path <- file.path(data.dir,"sales_ccity0milezipcode_sample.sas7bdat")
sales_cc_5miles_path <- file.path(data.dir,"sales_ccity5milezipcode_sample.sas7bdat")
sales_allother_zipcode_path <- file.path(data.dir,"sales_allotherzipcode_sample.sas7bdat")
```


```{r load data}
bb_zipcode <- read_sas(bb_zipcode_path) %>% 
  mutate_all(~as.factor(.))

sales_cc_0mile <- read_sas(sales_cc_0mile_path) %>% 
  mutate_at(vars(-pages_viewed, -duration,-event_date, -event_time,
                 -prod_name, -prod_qty, -prod_totprice, -basket_tot, 
                 -household_size, -hoh_oldest_age, -household_income, -children), as.factor)

sales_cc_5miles <- read_sas(sales_cc_5miles_path) %>% 
  mutate_at(vars(-pages_viewed, -duration,-event_date, -event_time,
                 -prod_name, -prod_qty, -prod_totprice, -basket_tot, 
                 -household_size, -hoh_oldest_age, -household_income, -children), as.factor)

# use encoding="latin1" in mac and linux, otherwise you don't need it
sales_allother_zipcode <- read_sas(sales_allother_zipcode_path, encoding = "latin1") %>% 
  mutate(Store_Close_Status = 0) %>% # NaN means no CC in 5-miles radius, we change NaN to 0
  mutate_at(vars(-pages_viewed, -duration,-event_date, -event_time,
                 -prod_name, -prod_qty, -prod_totprice, -basket_tot, 
                 -household_size, -hoh_oldest_age, -household_income, -children), as.factor)
```

```{r}
# Exclude Data without purchase
# All data should be with purchase -> tran_flg == 1
# tran_flg=1 in all datasets already we do not need to filter 
concat_data1 <- rbind(sales_allother_zipcode, sales_cc_0mile)
concat_data2 <- rbind(sales_allother_zipcode, sales_cc_5miles)
```


```{r}
# Filter Referring Domain

# we identify some search engines

ref_domain_to_consider1 <- c("", "GOOGLE.COM", "YAHOO.COM", "google.com", "yahoo.com",
                             "MSN.COM", "msn.com", "aol.com", "AOL.COM", "LIVE.COM", "live.com",
                             "MYWEBSEARCH.COM", "ASK.COM", "MYWAY.COM", "mywebsearch.com",
                             "ask.com", "YAHOO.NET", "BIZRATE.COM", "bizrate.com")

# Then we filter data by refer domain name
concat_data1 %<>% filter(ref_domain_name %in% ref_domain_to_consider1)
concat_data2 %<>% filter(ref_domain_name %in% ref_domain_to_consider1)
```

```{r}
# Filter Target Domain Name
five_target_domain_to_consider <- c("amazon.com", "staples.com", "dell.com", "walmart.com", "bestbuy.com")
two_target_domain_to_consider <- c("amazon.com","bestbuy.com")

# we can choose what filter to apply
concat_data1 %<>% filter(domain_name %in% five_target_domain_to_consider)
concat_data2 %<>% filter(domain_name %in% five_target_domain_to_consider)
```

```{r}
# Product Categories
# 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40
# remove 28, 30, 39, 40
# all_data$prod_category_id %>% unique() %>% sort
category_to_consider <- c(22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37)
experience_product <- c(24, 25, 26, 27, 28, 31, 32, 33, 34, 36, 37)
search_product <- c(22, 23, 24, 29, 30, 35)

concat_data1 %<>% 
  filter(prod_category_id %in% category_to_consider) %>% 
  mutate(prod_category_type = ifelse(prod_category_id %in% experience_product, 1, 0) %>% 
           as.factor()) #experience proudct=1, search product=0

concat_data2 %<>% 
  filter(prod_category_id %in% category_to_consider) %>% 
  mutate(prod_category_type = ifelse(prod_category_id %in% experience_product, 1, 0) %>% 
           as.factor()) #experience proudct=1, search product=0


```

```{r}
# construct MonthYear - month of year
concat_data1 %<>% mutate(MonthYear = format(event_date, "%Y-%m") )
concat_data2 %<>% mutate(MonthYear = format(event_date, "%Y-%m") )
```

```{r}
# Mark CC Closure

# CCStorePresent
# it is the same as Store_Close_Status
concat_data1 %<>% mutate(CCStorePresent = Store_Close_Status)
concat_data2 %<>% mutate(CCStorePresent = Store_Close_Status)
```

```{r}
# AfterStoreClosing
concat_data1 %<>% mutate(AfterStoreClosing = ifelse(MonthYear < "2008-11", 0, 1) %>% 
                           as.factor())
concat_data2 %<>% mutate(AfterStoreClosing = ifelse(MonthYear < "2008-11", 0, 1) %>% 
                           as.factor())  
```

```{r}
# BBStorePresent
concat_data1 %<>% 
  merge(.,bb_zipcode, by.x ="Zip_Code", by.y = "Zip_Code", all.x = TRUE) %>% 
  mutate(BBStorePresent = ifelse(is.na(BB_Store_Status), 0, BB_Store_Status) %>% 
           as.factor,
         PagesPerDollar = pages_viewed/prod_totprice,
         MinsPerDollar = duration/prod_totprice) 

concat_data2 %<>% 
  merge(.,bb_zipcode, by.x ="Zip_Code", by.y = "Zip_Code", all.x = TRUE) %>% 
  mutate(BBStorePresent = ifelse(is.na(BB_Store_Status), 0, BB_Store_Status) %>% 
           as.factor,
         PagesPerDollar = pages_viewed/prod_totprice,
         MinsPerDollar = duration/prod_totprice)
```

```{r}

data_0m_t5 <- sqldf("SELECT Zip_Code, MonthYear, domain_name, SUM(pages_viewed) / SUM(prod_totprice) AS PagesPerDollar, SUM(duration) / SUM(prod_totprice) AS MinsPerDollar, AVG(CCStorePresent) AS CCStorePresent, AVG(BBStorePresent) AS BBStorePresent, AVG(AfterStoreClosing) AS AfterStoreClosing FROM concat_data1 GROUP BY Zip_Code, MonthYear, domain_name")

concat_data1
```

```{r}
cf_d1 <- concat_data1 %>% 
   mutate(treatment=ifelse(CCStorePresent==1 & AfterStoreClosing==1, 1, 0)) %>% 
  select(-Store_Close_Status,-domain_id, -ref_domain_name,
         -event_date,-event_time,-tran_flg,-prod_name, -MonthYear,
         -CCStorePresent,- AfterStoreClosing,-BB_Store_Status,
         -site_session_id, -prod_category_id, -basket_tot, -machine_id, -Zip_Code)

cf_d2 <- concat_data2  %>%
  mutate(treatment=ifelse(CCStorePresent==1 & AfterStoreClosing==1, 1, 0)) %>% 
  select(-Store_Close_Status,-domain_id, -ref_domain_name,
         -event_date,-event_time,-tran_flg,-prod_name, -MonthYear,
         -CCStorePresent, -AfterStoreClosing,-BB_Store_Status,
         -site_session_id,-prod_category_id, -basket_tot, -machine_id, -Zip_Code)

```

```{r}
set.seed(1)

ama_cf_d1 <- cf_d1 %>% 
  filter(domain_name=="amazon.com") %>% 
  select(-domain_name)

W1_ama <- ama_cf_d1$treatment 
Y1_ama <- {ama_cf_d1$prod_totprice +1}%>% log(.)

d1_ama <- ama_cf_d1 %>% 
  select(-pages_viewed, -duration, -prod_qty, 
         -prod_totprice, -household_size, 
         -hoh_oldest_age, -household_income, 
         -children,-treatment)

d1_ama_exp <-model.matrix(~.+0, d1_ama)


# ama_cf_d1[,-"prod_totprice"]
X1_ama <- cbind(ama_cf_d1[,-c(4, which(colnames(ama_cf_d1) %in% colnames(d1_ama)))], d1_ama_exp)

Y1_f_ama <- regression_forest(X1_ama, Y1_ama)
Y1_hat_ama <- predict(Y1_f_ama)$predictions

W1_f_ama <- regression_forest(X1_ama, W1_ama)
W1_hat_ama <- predict(W1_f_ama)$predictions

cf1_raw_ama <- causal_forest(X1_ama, Y1_ama, W1_ama,
                       Y.hat = Y1_hat_ama, W.hat = W1_hat_ama)

varimp1_ama <- variable_importance(cf1_raw_ama)
selected1_idx_ama <- which(varimp1_ama > mean(varimp1_ama))

cf1_ama <- causal_forest(X1_ama[,selected1_idx_ama], Y1_ama, W1_ama,
                   Y.hat = Y1_hat_ama, W.hat = W1_hat_ama,
                   tune.parameters = "all")

tau1_hat_ama <- predict(cf1_ama)$predictions

```

```{r}
#
# Estimate ATE
#

ATE1_ama = average_treatment_effect(cf1_ama)
paste("95% CI for the ATE:", round(ATE1_ama[1], 3),
      "+/-", round(qnorm(0.975) * ATE1_ama[2], 3))

#
# Omnibus tests for heterogeneity
#

# Run best linear predictor analysis
test_calibration(cf1_ama)

# Compare regions with high and low estimated CATEs
high1_effect_ama = tau1_hat_ama > median(tau1_hat_ama)
ate1.high_ama = average_treatment_effect(cf1_ama, subset = high1_effect_ama, target.sample = "overlap")
ate1_low_ama = average_treatment_effect(cf1_ama, subset = !high1_effect_ama, target.sample = "overlap")
paste("95% CI for difference in ATE:",
      round(ate1.high_ama[1] - ate1_low_ama[1], 3), "+/-",
      round(qnorm(0.975) * sqrt(ate1.high_ama[2]^2 + ate1_low_ama[2]^2), 3))


```

```{r}
#
# Analaysis without fitting the propensity score
#

cf1_ama_noprop = causal_forest(X1_ama[,selected1_idx_ama], Y1_ama, W1_ama,
                          Y.hat = Y1_hat_ama, W.hat = mean(W1_ama),
                          tune.parameters = "all")
tau1_hat_ama_noprop = predict(cf1_ama_noprop)$predictions

ATE1_ama_noprop = average_treatment_effect(cf1_ama_noprop)
paste("95% CI for the ATE:", round(ATE1_ama_noprop[1], 3),
      "+/-", round(qnorm(0.975) * ATE1_ama_noprop[2], 3))

pdf("tauhat1_ama_noprop.pdf")
pardef = par(mar = c(5, 4, 4, 2) + 0.5, cex.lab=1.5, cex.axis=1.5, cex.main=1.5, cex.sub=1.5)
plot(tau1_hat_ama, tau1_hat_ama_noprop,
     xlim = range(tau1_hat_ama, tau1_hat_ama_noprop),
     ylim = range(tau1_hat_ama, tau1_hat_ama_noprop),
     xlab = "orthogonalized causal forest estimates",
     ylab = "non-orthogonalized causal forest")
abline(0, 1, lwd = 2, lty = 2, col = 4)
par = pardef
dev.off()




```

```{r}
#
# Make some plots...
#

pdf("tauhat1_ama_hist.pdf")
pardef = par(mar = c(5, 4, 4, 2) + 0.5, cex.lab=1.5, cex.axis=1.5, cex.main=1.5, cex.sub=1.5)
hist(tau1_hat_ama, xlab = "estimated CATE", main = "")
dev.off()

pdf("tauhat1_ama_hist_noprop.pdf")
pardef = par(mar = c(5, 4, 4, 2) + 0.5, cex.lab=1.5, cex.axis=1.5, cex.main=1.5, cex.sub=1.5)
hist(tau1_hat_ama_noprop, xlab = "estimated CATE", main = "")
dev.off()

pdf("tauhat1_ama_vs_dur.pdf")
pardef = par(mar = c(5, 4, 4, 2) + 0.5, cex.lab=1.5, cex.axis=1.5, cex.main=1.5, cex.sub=1.5)
plot(tau1_hat_ama ~ round(X1_ama$duration), xlab = "duration", ylab = "estimated CATE")
lines(smooth.spline(4 + X1_ama[,"duration"], tau1_hat_ama, df = 4), lwd = 2, col = 4)
dev.off()
```

```{r}
set.seed(1)

ama_cf_d2 <- cf_d2 %>% 
  filter(domain_name=="amazon.com") %>% 
  select(-domain_name)

W2_ama <- ama_cf_d2$treatment 
Y2_ama <- {ama_cf_d2$prod_totprice +1}%>% log(.)

d2_ama <- ama_cf_d2 %>% 
  select(-pages_viewed, -duration, -prod_qty, 
         -prod_totprice, -household_size, 
         -hoh_oldest_age, -household_income, 
         -children,-treatment)

d2_ama_exp <-model.matrix(~.+0, d2_ama)

X2_ama <- cbind(ama_cf_d2[,-c(4, which(colnames(ama_cf_d2) %in% colnames(d2_ama)))], d2_ama_exp)

Y2_f_ama <- regression_forest(X2_ama, Y2_ama)
Y2_hat_ama <- predict(Y2_f_ama)$predictions

W2_f_ama <- regression_forest(X2_ama, W2_ama)
W2_hat_ama <- predict(W2_f_ama)$predictions

cf2_raw_ama <- causal_forest(X2_ama, Y2_ama, W2_ama,
                       Y.hat = Y2_hat_ama, W.hat = W2_hat_ama)

varimp2_ama <- variable_importance(cf2_raw_ama)
selected2_idx_ama <- which(varimp2_ama > mean(varimp2_ama))

cf2_ama <- causal_forest(X2_ama[,selected2_idx_ama], Y2_ama, W2_ama,
                   Y.hat = Y2_hat_ama, W.hat = W2_hat_ama,
                   tune.parameters = "all")

tau2_hat_ama <- predict(cf2_ama)$predictions

```

```{r}
#
# Estimate ATE
#

ATE2_ama = average_treatment_effect(cf2_ama, target.sample = "overlap")
paste("95% CI for the ATE:", round(ATE2_ama[1], 3),
      "+/-", round(qnorm(0.975) * ATE2_ama[2], 3))

#
# Omnibus tests for heterogeneity
#

# Run best linear predictor analysis
test_calibration(cf2_ama)

# Compare regions with high and low estimated CATEs
high2_effect_ama = tau2_hat_ama > median(tau2_hat_ama)
ate2.high_ama = average_treatment_effect(cf2_ama, subset = high2_effect_ama, target.sample = "overlap")
ate2_low_ama = average_treatment_effect(cf2_ama, subset = !high2_effect_ama, target.sample = "overlap")
paste("95% CI for difference in ATE:",
      round(ate2.high_ama[1] - ate2_low_ama[1], 3), "+/-",
      round(qnorm(0.975) * sqrt(ate2.high_ama[2]^2 + ate2_low_ama[2]^2), 3))


```

```{r}
#
# Analaysis without fitting the propensity score
#

cf2_ama_noprop = causal_forest(X2_ama[,selected2_idx_ama], Y2_ama, W2_ama,
                          Y.hat = Y2_hat_ama, W.hat = mean(W2_ama),
                          tune.parameters = "all")
tau2_hat_ama_noprop = predict(cf2_ama_noprop)$predictions

ATE2_ama_noprop = average_treatment_effect(cf2_ama_noprop)
paste("95% CI for the ATE:", round(ATE2_ama_noprop[1], 3),
      "+/-", round(qnorm(0.975) * ATE2_ama_noprop[2], 3))

pdf("tauhat2_ama_noprop.pdf")
pardef = par(mar = c(5, 4, 4, 2) + 0.5, cex.lab=1.5, cex.axis=1.5, cex.main=1.5, cex.sub=1.5)
plot(tau2_hat_ama, tau2_hat_ama_noprop,
     xlim = range(tau2_hat_ama, tau2_hat_ama_noprop),
     ylim = range(tau2_hat_ama, tau2_hat_ama_noprop),
     xlab = "orthogonalized causal forest estimates",
     ylab = "non-orthogonalized causal forest")
abline(0, 1, lwd = 2, lty = 2, col = 4)
par = pardef
dev.off()




```

```{r}
#
# Make some plots...
#

pdf("tauhat2_ama_hist.pdf")
pardef = par(mar = c(5, 4, 4, 2) + 0.5, cex.lab=1.5, cex.axis=1.5, cex.main=1.5, cex.sub=1.5)
hist(tau2_hat_ama, xlab = "estimated CATE", main = "")
dev.off()

pdf("tauhat2_ama_hist_noprop.pdf")
pardef = par(mar = c(5, 4, 4, 2) + 0.5, cex.lab=1.5, cex.axis=1.5, cex.main=1.5, cex.sub=1.5)
hist(tau2_hat_ama_noprop, xlab = "estimated CATE", main = "")
dev.off()

pdf("tauhat2_ama_vs_pagesview.pdf")
pardef = par(mar = c(5, 4, 4, 2) + 0.5, cex.lab=1.5, cex.axis=1.5, cex.main=1.5, cex.sub=1.5)
plot(tau2_hat_ama ~ round(X2_ama$pages_viewed), xlab = "pages_viewed", ylab = "estimated CATE")
lines(smooth.spline(4 + X2_ama[,"pages_viewed"], tau2_hat_ama, df = 4), lwd = 2, col = 4)
dev.off()
```

```{r}
set.seed(1)

bb_cf_d1 <- cf_d1 %>% 
  filter(domain_name=="bestbuy.com") %>% 
  select(-domain_name)

W1_bb <- bb_cf_d1$treatment 
Y1_bb <- {bb_cf_d1$prod_totprice +1}%>% log(.)

d1_bb <- bb_cf_d1 %>% 
  select(-pages_viewed, -duration, -prod_qty, 
         -prod_totprice, -household_size, 
         -hoh_oldest_age, -household_income, 
         -children,-treatment)

d1_bb_exp <-model.matrix(~.+0, d1_bb)

X1_bb <- cbind(bb_cf_d1[,-c(4, which(colnames(bb_cf_d1) %in% colnames(d1_bb)))], d1_bb_exp)

Y1_f_bb <- regression_forest(X1_bb, Y1_bb)
Y1_hat_bb <- predict(Y1_f_bb)$predictions

W1_f_bb <- regression_forest(X1_bb, W1_bb)
W1_hat_bb <- predict(W1_f_bb)$predictions

cf1_raw_bb <- causal_forest(X1_bb, Y1_bb, W1_bb,
                       Y.hat = Y1_hat_bb, W.hat = W1_hat_bb)

varimp1_bb <- variable_importance(cf1_raw_bb)
selected1_idx_bb <- which(varimp1_bb > mean(varimp1_bb))

cf1_bb <- causal_forest(X1_bb[,selected1_idx_bb], Y1_bb, W1_bb,
                   Y.hat = Y1_hat_bb, W.hat = W1_hat_bb,
                   tune.parameters = "all")

tau1_hat_bb <- predict(cf1_bb)$predictions


```

```{r}
#
# Estimate ATE
#

ATE1_bb = average_treatment_effect(cf1_bb, target.sample = "overlap")
paste("95% CI for the ATE:", round(ATE1_bb[1], 3),
      "+/-", round(qnorm(0.975) * ATE1_bb[2], 3))

#
# Omnibus tests for heterogeneity
#

# Run best linear predictor analysis
test_calibration(cf1_bb)

# Compare regions with high and low estimated CATEs
high1_effect_bb = tau1_hat_bb > median(tau1_hat_bb)
ate1.high_bb = average_treatment_effect(cf1_bb, subset = high1_effect_bb, target.sample = "overlap")
ate1_low_bb = average_treatment_effect(cf1_bb, subset = !high1_effect_bb, target.sample = "overlap")
paste("95% CI for difference in ATE:",
      round(ate1.high_ama[1] - ate1_low_bb[1], 3), "+/-",
      round(qnorm(0.975) * sqrt(ate1.high_bb[2]^2 + ate1_low_bb[2]^2), 3))


```

```{r}
#
# Analaysis without fitting the propensity score
#

cf1_bb_noprop = causal_forest(X1_bb[,selected1_idx_bb], Y1_bb, W1_bb,
                          Y.hat = Y1_hat_bb, W.hat = mean(W1_bb),
                          tune.parameters = "all")
tau1_hat_bb_noprop = predict(cf1_bb_noprop)$predictions

ATE1_bb_noprop = average_treatment_effect(cf1_bb_noprop)
paste("95% CI for the ATE:", round(ATE1_bb_noprop[1], 3),
      "+/-", round(qnorm(0.975) * ATE1_bb_noprop[2], 3))

pdf("tauhat1_bb_noprop.pdf")
pardef = par(mar = c(5, 4, 4, 2) + 0.5, cex.lab=1.5, cex.axis=1.5, cex.main=1.5, cex.sub=1.5)
plot(tau1_hat_bb, tau1_hat_bb_noprop,
     xlim = range(tau1_hat_bb, tau1_hat_bb_noprop),
     ylim = range(tau1_hat_bb, tau1_hat_bb_noprop),
     xlab = "orthogonalized causal forest estimates",
     ylab = "non-orthogonalized causal forest")
abline(0, 1, lwd = 2, lty = 2, col = 4)
par = pardef
dev.off()




```

```{r}
#
# Make some plots...
#

pdf("tauhat1_bb_hist.pdf")
pardef = par(mar = c(5, 4, 4, 2) + 0.5, cex.lab=1.5, cex.axis=1.5, cex.main=1.5, cex.sub=1.5)
hist(tau1_hat_bb, xlab = "estimated CATE", main = "")
dev.off()

pdf("tauhat1_bb_hist_noprop.pdf")
pardef = par(mar = c(5, 4, 4, 2) + 0.5, cex.lab=1.5, cex.axis=1.5, cex.main=1.5, cex.sub=1.5)
hist(tau1_hat_bb_noprop, xlab = "estimated CATE", main = "")
dev.off()

pdf("tauhat1_bb_vs_pagesview.pdf")
pardef = par(mar = c(5, 4, 4, 2) + 0.5, cex.lab=1.5, cex.axis=1.5, cex.main=1.5, cex.sub=1.5)
plot(tau1_hat_bb ~ round(X1_bb$pages_viewed), xlab = "pages_viewed", ylab = "estimated CATE")
lines(smooth.spline(4 + X1_bb[,"pages_viewed"], tau1_hat_bb, df = 4), lwd = 2, col = 4)
dev.off()
```

```{r}
set.seed(1)

bb_cf_d2 <- cf_d2 %>% 
  filter(domain_name=="bestbuy.com") %>% 
  select(-domain_name)


W2_bb <- bb_cf_d2$treatment 
Y2_bb <- {bb_cf_d2$prod_totprice +1}%>% log(.)

d2_bb <- bb_cf_d2 %>% 
  select(-pages_viewed, -duration, -prod_qty, 
         -prod_totprice, -household_size, 
         -hoh_oldest_age, -household_income, 
         -children,-treatment)

d2_bb_exp <-model.matrix(~.+0, d2_bb)

X2_bb <- cbind(bb_cf_d2[,-c(4, which(colnames(bb_cf_d2) %in% colnames(d2_bb)))], d2_bb_exp)

Y2_f_bb <- regression_forest(X2_bb, Y2_bb)
Y2_hat_bb <- predict(Y2_f_bb)$predictions

W2_f_bb <- regression_forest(X2_bb, W2_bb)
W2_hat_bb <- predict(W2_f_bb)$predictions

cf2_raw_bb <- causal_forest(X2_bb, Y2_bb, W2_bb,
                       Y.hat = Y2_hat_bb, W.hat = W2_hat_bb)

varimp2_bb <- variable_importance(cf2_raw_bb)
selected2_idx_bb <- which(varimp2_bb > mean(varimp2_bb))

cf2_bb <- causal_forest(X2_bb[,selected2_idx_bb], Y2_bb, W2_bb,
                   Y.hat = Y2_hat_bb, W.hat = W2_hat_bb,
                   tune.parameters = "all")

tau2_hat_bb <- predict(cf2_bb)$predictions


```

```{r}
#
# Estimate ATE
#

ATE2_bb = average_treatment_effect(cf2_bb, target.sample = "overlap")
paste("95% CI for the ATE:", round(ATE2_bb[1], 3),
      "+/-", round(qnorm(0.975) * ATE2_bb[2], 3))

#
# Omnibus tests for heterogeneity
#

# Run best linear predictor analysis
test_calibration(cf2_bb)

# Compare regions with high and low estimated CATEs
high2_effect_bb = tau2_hat_bb > median(tau2_hat_bb)
ate2.high_bb = average_treatment_effect(cf2_bb, subset = high2_effect_bb, target.sample = "overlap")
ate2_low_bb = average_treatment_effect(cf2_bb, subset = !high2_effect_bb, target.sample = "overlap")
paste("95% CI for difference in ATE:",
      round(ate2.high_ama[1] - ate2_low_bb[1], 3), "+/-",
      round(qnorm(0.975) * sqrt(ate2.high_bb[2]^2 + ate2_low_bb[2]^2), 3))


```

```{r}
#
# Analaysis without fitting the propensity score
#

cf2_bb_noprop = causal_forest(X2_bb[,selected2_idx_bb], Y2_bb, W2_bb,
                          Y.hat = Y2_hat_bb, W.hat = mean(W2_bb),
                          tune.parameters = "all")
tau2_hat_bb_noprop = predict(cf2_bb_noprop)$predictions

ATE2_bb_noprop = average_treatment_effect(cf2_bb_noprop)
paste("95% CI for the ATE:", round(ATE2_bb_noprop[1], 3),
      "+/-", round(qnorm(0.975) * ATE2_bb_noprop[2], 3))

pdf("tauhat2_bb_noprop.pdf")
pardef = par(mar = c(5, 4, 4, 2) + 0.5, cex.lab=1.5, cex.axis=1.5, cex.main=1.5, cex.sub=1.5)
plot(tau2_hat_bb, tau2_hat_bb_noprop,
     xlim = range(tau2_hat_bb, tau2_hat_bb_noprop),
     ylim = range(tau2_hat_bb, tau2_hat_bb_noprop),
     xlab = "orthogonalized causal forest estimates",
     ylab = "non-orthogonalized causal forest")
abline(0, 1, lwd = 2, lty = 2, col = 4)
par = pardef
dev.off()




```

```{r}
#
# Make some plots...
#

pdf("tauhat2_bb_hist.pdf")
pardef = par(mar = c(5, 4, 4, 2) + 0.5, cex.lab=1.5, cex.axis=1.5, cex.main=1.5, cex.sub=1.5)
hist(tau2_hat_bb, xlab = "estimated CATE", main = "")
dev.off()

pdf("tauhat2_bb_hist_noprop.pdf")
pardef = par(mar = c(5, 4, 4, 2) + 0.5, cex.lab=1.5, cex.axis=1.5, cex.main=1.5, cex.sub=1.5)
hist(tau2_hat_bb_noprop, xlab = "estimated CATE", main = "")
dev.off()

pdf("tauhat2_bb_vs_pagesview.pdf")
pardef = par(mar = c(5, 4, 4, 2) + 0.5, cex.lab=1.5, cex.axis=1.5, cex.main=1.5, cex.sub=1.5)
plot(tau2_hat_bb ~ round(X2_bb$pages_viewed), xlab = "pages_viewed", ylab = "estimated CATE")
lines(smooth.spline(4 + X2_bb[,"pages_viewed"], tau2_hat_bb, df = 4), lwd = 2, col = 4)
dev.off()
```




